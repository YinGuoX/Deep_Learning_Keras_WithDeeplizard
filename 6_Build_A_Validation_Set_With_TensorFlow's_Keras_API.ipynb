{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6_Build A Validation Set With TensorFlow's Keras API.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNXfW35LAfMu4hOpjWJYcOC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YinGuoX/Deep_Learning_Keras_WithDeeplizard/blob/master/6_Build_A_Validation_Set_With_TensorFlow's_Keras_API.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FC2Ynd8RKaao"
      },
      "source": [
        "# Build A Validation Set With TensorFlow's Keras API\r\n",
        "在本集中，我们将演示如何在训练期间使用TensorFlow的Keras API即时创建验证集。\r\n",
        "\r\n",
        "我们将继续使用在上一集中构建和训练的相同模型，但首先，让我们讨论一下验证集到底是什么。\r\n",
        "\r\n",
        "## 1.什么是验证集\r\n",
        "\r\n",
        "回想一下，我们以前建立了一个训练集，在这个训练集上训练我们的模型。随着我们的模型被训练的每一个epoch，模型将继续学习这个训练集中数据的特征和特征。\r\n",
        "\r\n",
        "我们的希望是，以后我们可以利用这个模型，将它应用到新的数据中，并且让这个模型仅仅基于它从训练集中学到的东西，就能准确地预测它以前从未见过的数据。\r\n",
        "\r\n",
        "现在，让我们讨论一下添加验证集的作用。\r\n",
        "\r\n",
        "在训练开始之前，我们可以选择删除一部分训练集并将其放入验证集中。 然后，在训练期间，模型将仅在训练集上进行训练，并且它将通过评估验证集中的数据进行验证。\r\n",
        "\r\n",
        "本质上，该模型是在训练集中学习数据的特征，从该数据中获取所学信息，然后在验证集中进行预测。 在每个时期，我们不仅会看到训练集的损失和准确性结果，还会看到验证集的损失和准确性结果。\r\n",
        "\r\n",
        "这样一来，我们就可以了解该模型对未经训练的数据进行概括的程度，因为回想一下，验证数据不应该作为训练数据的一部分。\r\n",
        "\r\n",
        "这也有助于我们查看模型是否过拟合。 当模型仅学习训练数据的细节并且无法很好地归纳未训练的数据时，就会发生过拟合。\r\n",
        "\r\n",
        "如果您想进一步了解过度拟合问题，请查看“深度学习基础知识”系列中的过度拟合一集。 请注意，您还可以在该系列中看到训练集和验证集的更深入细分。\r\n",
        "\r\n",
        "现在让我们讨论一下如何创建验证集。\r\n",
        "\r\n",
        "## 2. 创建验证集\r\n",
        "有两种方法可以创建用于tf.keras.Sequential模型的验证集。\r\n",
        "\r\n",
        "### 2.1 手工创建验证集\r\n",
        "第一种方法是创建一个数据结构来保存验证集，并按照与训练集相同的方式将数据直接放入该结构中。\r\n",
        "\r\n",
        "此数据结构应为Numpy数组或张量的元组valid_set =（x_val，y_val），其中x_val是包含验证样本的numpy数组或张量，而y_val是包含验证标签的numpy数组或张量。\r\n",
        "\r\n",
        "当我们调用model.fit（）时，除了训练集之外，我们还将传入验证集。 我们通过指定validation_data参数来传递验证集。\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "```\r\n",
        "model.fit(\r\n",
        "      x=scaled_train_samples\r\n",
        "    , y=train_labels\r\n",
        "    , validation_data=valid_set\r\n",
        "    , batch_size=10\r\n",
        "    , epochs=30\r\n",
        "    , verbose=2\r\n",
        ")\r\n",
        "```\r\n",
        "当模型进行训练时，它将继续只在训练集上进行训练，但除此之外，它还将评估验证集。\r\n",
        "\r\n",
        "\r\n",
        "### 2.2 使用Keras创建验证集\r\n",
        "创建验证集的另一种方法可以节省步骤！\r\n",
        "\r\n",
        "如果尚未创建指定的验证集，则在调用model.fit（）时，可以为validation_split参数设置一个值。 它期望分数在0到1之间。假设我们将此参数设置为0.1。\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "```\r\n",
        "model.fit(\r\n",
        "      x=scaled_train_samples\r\n",
        "    , y=train_labels\r\n",
        "    , validation_split=0.1\r\n",
        "    , batch_size=10\r\n",
        "    , epochs=30\r\n",
        "    , verbose=2\r\n",
        ")\r\n",
        "```\r\n",
        "指定此参数后，Keras将分割训练数据的一部分（在此示例中为10％），以用作验证数据。 模型将分开训练数据的这一部分，不对其进行训练，并且将在每个时期结束时评估此数据的损失和任何模型度量。\r\n",
        "\r\n",
        "**注意：**默认情况下，fit（）函数会在每个时期之前对数据进行随机排序。 但是，当指定validation_split参数时，将在混洗之前从x和y数据中的最后一个样本中选择验证数据\r\n",
        "\r\n",
        "因此，在我们以这种方式使用validation_split创建验证数据的情况下，我们需要确保我们的数据已提前打乱，就像我们先前在较早的情节中所做的那样。\r\n",
        "\r\n",
        "## 3.Interpret Validation Metrics\r\n",
        "现在,无论哪一种方法我们使用创建验证数据,当我们调用model.fit(),那么除了损失和准确性被显示为每个时代为我们上次看到的,我们现在也看到val损失和val acc追踪损失和准确性的验证集"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IWO50-UKW1U",
        "outputId": "c32d61c7-739c-4285-eb4f-1d53875bf2bc"
      },
      "source": [
        "# 准备数据\r\n",
        "import numpy as np\r\n",
        "from random import randint\r\n",
        "from sklearn.utils import shuffle\r\n",
        "from sklearn.preprocessing import MinMaxScaler\r\n",
        "\r\n",
        "train_labels = []\r\n",
        "train_samples = []\r\n",
        "\r\n",
        "# 生成数据\r\n",
        "for i in range(50):\r\n",
        "    # 大约5%的年轻人确实经历过副作用\r\n",
        "    random_younger = randint(13,64)\r\n",
        "    train_samples.append(random_younger)\r\n",
        "    train_labels.append(1)\r\n",
        "\r\n",
        "    # 大约5%的老年人没有经历过副作用\r\n",
        "    random_older = randint(65,100)\r\n",
        "    train_samples.append(random_older)\r\n",
        "    train_labels.append(0)\r\n",
        "\r\n",
        "for i in range(1000):\r\n",
        "    # 大约95%的年轻人没有经历过副作用\r\n",
        "    random_younger = randint(13,64)\r\n",
        "    train_samples.append(random_younger)\r\n",
        "    train_labels.append(0)\r\n",
        "\r\n",
        "    # 大约95%的老年人确实经历过副作用\r\n",
        "    random_older = randint(65,100)\r\n",
        "    train_samples.append(random_older)\r\n",
        "    train_labels.append(1)\r\n",
        "\r\n",
        "train_labels = np.array(train_labels)\r\n",
        "train_samples = np.array(train_samples)\r\n",
        "train_labels,train_samples = shuffle(train_labels,train_samples)\r\n",
        "print(train_samples.shape)\r\n",
        "\r\n",
        "# 通过将每个特征缩放到给定的范围来转换特征。\r\n",
        "scaler = MinMaxScaler(feature_range=(0,1))\r\n",
        "\r\n",
        "scaled_train_samples = scaler.fit_transform(train_samples.reshape(-1,1))\r\n",
        "# 我们只是根据fit_transform（）函数默认情况下不接受一维数据的情况，将数据重塑为2D。\r\n",
        "print(train_samples.reshape(-1,1).shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2100,)\n",
            "(2100, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtlnfb05NTHJ"
      },
      "source": [
        "# 创建并且编译模型\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow import keras\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras.layers import Activation, Dense\r\n",
        "from tensorflow.keras.optimizers import Adam\r\n",
        "from tensorflow.keras.metrics import categorical_crossentropy\r\n",
        "\r\n",
        "model = Sequential([\r\n",
        "    Dense(units=16, input_shape=(1,), activation='relu'),\r\n",
        "    Dense(units=32, activation='relu'),\r\n",
        "    Dense(units=2, activation='softmax')\r\n",
        "])\r\n",
        "\r\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001),\r\n",
        "              loss='sparse_categorical_crossentropy',\r\n",
        "              metrics=['accuracy'])\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQ6feGpgNddB",
        "outputId": "b75e5263-1ac8-4e78-b14f-be4a5dd1ac60"
      },
      "source": [
        "# 训练模型\r\n",
        "model.fit(\r\n",
        "    x = scaled_train_samples,\r\n",
        "    y = train_labels,\r\n",
        "    validation_split=0.1,\r\n",
        "    batch_size = 10,\r\n",
        "    epochs = 30,\r\n",
        "    verbose=2\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "189/189 - 1s - loss: 0.6783 - accuracy: 0.5968 - val_loss: 0.6701 - val_accuracy: 0.5810\n",
            "Epoch 2/30\n",
            "189/189 - 0s - loss: 0.6506 - accuracy: 0.6619 - val_loss: 0.6476 - val_accuracy: 0.6143\n",
            "Epoch 3/30\n",
            "189/189 - 0s - loss: 0.6234 - accuracy: 0.7201 - val_loss: 0.6240 - val_accuracy: 0.6524\n",
            "Epoch 4/30\n",
            "189/189 - 0s - loss: 0.5965 - accuracy: 0.7492 - val_loss: 0.5999 - val_accuracy: 0.6952\n",
            "Epoch 5/30\n",
            "189/189 - 0s - loss: 0.5673 - accuracy: 0.7820 - val_loss: 0.5713 - val_accuracy: 0.7476\n",
            "Epoch 6/30\n",
            "189/189 - 0s - loss: 0.5372 - accuracy: 0.8122 - val_loss: 0.5449 - val_accuracy: 0.7524\n",
            "Epoch 7/30\n",
            "189/189 - 0s - loss: 0.5082 - accuracy: 0.8259 - val_loss: 0.5172 - val_accuracy: 0.7810\n",
            "Epoch 8/30\n",
            "189/189 - 0s - loss: 0.4801 - accuracy: 0.8460 - val_loss: 0.4907 - val_accuracy: 0.8095\n",
            "Epoch 9/30\n",
            "189/189 - 0s - loss: 0.4539 - accuracy: 0.8587 - val_loss: 0.4666 - val_accuracy: 0.8286\n",
            "Epoch 10/30\n",
            "189/189 - 0s - loss: 0.4296 - accuracy: 0.8683 - val_loss: 0.4425 - val_accuracy: 0.8667\n",
            "Epoch 11/30\n",
            "189/189 - 0s - loss: 0.4072 - accuracy: 0.8730 - val_loss: 0.4208 - val_accuracy: 0.8714\n",
            "Epoch 12/30\n",
            "189/189 - 0s - loss: 0.3871 - accuracy: 0.8889 - val_loss: 0.4022 - val_accuracy: 0.8714\n",
            "Epoch 13/30\n",
            "189/189 - 0s - loss: 0.3696 - accuracy: 0.8931 - val_loss: 0.3829 - val_accuracy: 0.8857\n",
            "Epoch 14/30\n",
            "189/189 - 0s - loss: 0.3539 - accuracy: 0.8984 - val_loss: 0.3658 - val_accuracy: 0.8857\n",
            "Epoch 15/30\n",
            "189/189 - 0s - loss: 0.3406 - accuracy: 0.9085 - val_loss: 0.3521 - val_accuracy: 0.9000\n",
            "Epoch 16/30\n",
            "189/189 - 0s - loss: 0.3290 - accuracy: 0.9148 - val_loss: 0.3408 - val_accuracy: 0.9000\n",
            "Epoch 17/30\n",
            "189/189 - 0s - loss: 0.3190 - accuracy: 0.9175 - val_loss: 0.3303 - val_accuracy: 0.9000\n",
            "Epoch 18/30\n",
            "189/189 - 0s - loss: 0.3105 - accuracy: 0.9190 - val_loss: 0.3201 - val_accuracy: 0.9190\n",
            "Epoch 19/30\n",
            "189/189 - 0s - loss: 0.3031 - accuracy: 0.9228 - val_loss: 0.3112 - val_accuracy: 0.9190\n",
            "Epoch 20/30\n",
            "189/189 - 0s - loss: 0.2968 - accuracy: 0.9243 - val_loss: 0.3035 - val_accuracy: 0.9190\n",
            "Epoch 21/30\n",
            "189/189 - 0s - loss: 0.2914 - accuracy: 0.9302 - val_loss: 0.2976 - val_accuracy: 0.9190\n",
            "Epoch 22/30\n",
            "189/189 - 0s - loss: 0.2870 - accuracy: 0.9296 - val_loss: 0.2928 - val_accuracy: 0.9190\n",
            "Epoch 23/30\n",
            "189/189 - 0s - loss: 0.2828 - accuracy: 0.9270 - val_loss: 0.2853 - val_accuracy: 0.9238\n",
            "Epoch 24/30\n",
            "189/189 - 0s - loss: 0.2796 - accuracy: 0.9328 - val_loss: 0.2827 - val_accuracy: 0.9238\n",
            "Epoch 25/30\n",
            "189/189 - 0s - loss: 0.2766 - accuracy: 0.9328 - val_loss: 0.2794 - val_accuracy: 0.9238\n",
            "Epoch 26/30\n",
            "189/189 - 0s - loss: 0.2739 - accuracy: 0.9328 - val_loss: 0.2763 - val_accuracy: 0.9238\n",
            "Epoch 27/30\n",
            "189/189 - 0s - loss: 0.2714 - accuracy: 0.9312 - val_loss: 0.2709 - val_accuracy: 0.9286\n",
            "Epoch 28/30\n",
            "189/189 - 0s - loss: 0.2695 - accuracy: 0.9392 - val_loss: 0.2714 - val_accuracy: 0.9238\n",
            "Epoch 29/30\n",
            "189/189 - 0s - loss: 0.2677 - accuracy: 0.9344 - val_loss: 0.2676 - val_accuracy: 0.9238\n",
            "Epoch 30/30\n",
            "189/189 - 0s - loss: 0.2660 - accuracy: 0.9354 - val_loss: 0.2649 - val_accuracy: 0.9286\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb8d9007710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ck9JruhUN2_J"
      },
      "source": [
        "现在，我们不仅可以看到我们的模型对训练数据的特征的学习程度如何，还可以看到模型对验证集中的新的，看不见的数据的概括程度。 接下来，我们将看到如何使用我们的模型进行预测。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EglXdltCN47X"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}