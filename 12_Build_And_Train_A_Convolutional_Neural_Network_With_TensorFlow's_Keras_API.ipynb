{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "12_Build And Train A Convolutional Neural Network With TensorFlow's Keras API.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMi2SkhGqj7mtscseLSEf4u",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YinGuoX/Deep_Learning_Keras_WithDeeplizard/blob/master/12_Build_And_Train_A_Convolutional_Neural_Network_With_TensorFlow's_Keras_API.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTpQBX2UuTBw"
      },
      "source": [
        "# Build And Train A Convolutional Neural Network With TensorFlow's Keras API\r\n",
        "在本集中，我们将演示如何构建一个简单的卷积神经网络(CNN)，并使用TensorFlow的Keras API对猫和狗的图像进行训练。\r\n",
        "\r\n",
        "我们将使用我们在上一集准备的图像数据。一定要确保你已经完成了那个章节的第一步来获取和准备数据，同时也要确保你还有我们上次引入的所有导入，因为我们会继续在这里使用它们。\r\n",
        "\r\n",
        "## 1. 构建一个简单的CNN\r\n",
        "为了构建CNN，我们将使用Keras顺序模型。回想一下，我们在前面的章节中首先介绍了顺序模型。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "af7sZUCytMWO"
      },
      "source": [
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow import keras\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras.layers import Activation, Dense, Flatten, BatchNormalization, Conv2D, MaxPool2D\r\n",
        "from tensorflow.keras.optimizers import Adam\r\n",
        "from tensorflow.keras.metrics import categorical_crossentropy\r\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n",
        "from sklearn.metrics import confusion_matrix\r\n",
        "import itertools\r\n",
        "import os\r\n",
        "import shutil\r\n",
        "import random\r\n",
        "import glob\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import warnings\r\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\r\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOYwQfo0uwR8"
      },
      "source": [
        "model = Sequential([\r\n",
        "    Conv2D(filters=32,kernel_size=(3,3),activation='relu',padding='same',input_shape=(224,224,3)),\r\n",
        "    # strides：步幅\r\n",
        "    MaxPool2D(pool_size=(2,2),strides=2),\r\n",
        "    Conv2D(filters=64,kernel_size=(3,3),activation='relu',padding='same'),\r\n",
        "    MaxPool2D(pool_size=(2,2),strides=2),\r\n",
        "    Flatten(),\r\n",
        "    Dense(units=2,activation='softmax')\r\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6voHmIhvZ-5"
      },
      "source": [
        "模型的第一层是一个二维卷积层。这一层将有32个输出滤波器，每个滤波器的内核大小为3x3，我们将使用relu激活函数。\r\n",
        "\r\n",
        "**注意：**指定的输出滤波器数量的选择是任意的，所选择的核大小3x3通常是非常常用的大小。您可以通过为这些参数选择不同的值来进行试验。\r\n",
        "\r\n",
        "我们通过指定padding = 'same'来启用零填充。\r\n",
        "\r\n",
        "仅在第一层，我们还指定了输入形状，也就是数据的形状。我们的图像是224像素高和224像素宽，有3个颜色通道:RGB。这给了我们一个输入形状(224,224,3)。\r\n",
        "\r\n",
        "然后我们添加一个最大池层池和减少数据的维数。注意，为了获得对最大池化、零填充、卷积滤波器和卷积神经网络的基本理解，请查看深度学习基础课程。\r\n",
        "\r\n",
        "在此之后，我们添加另一个卷积层，其规格与前面的层完全相同，只是第二个Conv2D层有64个过滤器。这里选择64也是任意的，但是通常的选择是在后面的层中使用比前面的层更多的过滤器。这个层后面是相同类型的MaxPool2D层。\r\n",
        "\r\n",
        "然后我们将卷积层的输出压平，并将其传递到一个密集层。这个密集的层是网络的输出层，所以它有两个节点，一个为猫，一个为狗。我们将在输出上使用softmax激活函数，以便每个样本的输出是cat和dog输出的概率分布。\r\n",
        "\r\n",
        "我们可以通过调用model.summary()来查看模型的摘要。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZdv6ZZUvZfI",
        "outputId": "60c2a244-0318-487b-f2cc-9012e41c1769"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 224, 224, 32)      896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 112, 112, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 112, 112, 64)      18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 56, 56, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 200704)            0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 2)                 401410    \n",
            "=================================================================\n",
            "Total params: 420,802\n",
            "Trainable params: 420,802\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5YpRcA-wnVy"
      },
      "source": [
        "现在模型已经建立，我们使用Adam优化器编译模型，学习率为0.0001，分类交叉熵损失，我们将把准确性作为性能指标。同样，如果你需要对这些主题的基本理解，请查看深度学习基础课程。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RvgbxZ9wfr6"
      },
      "source": [
        "model.compile(optimizer=Adam(learning_rate=0.0001),loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Gpmn_xHw1g1"
      },
      "source": [
        "**注意:**当我们只有两个类时，我们可以将输出层配置为只有一个输出，而不是两个输出，并使用binary_crossentropy作为损失，而不是categorical_crossentropy。 两种选择均能很好地工作，并获得完全相同的结果。\r\n",
        "\r\n",
        "但是，对于binary_crossentropy，最后一层需要使用Sigmoid而不是softmax作为其激活函数。\r\n",
        "\r\n",
        "## 2. 训练一个简单的CNN\r\n",
        "首先导入前几集构建的数据集"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5YBJ4cqwu-v",
        "outputId": "7ae5c773-53e5-4b11-e58a-386df8af1dbf"
      },
      "source": [
        "# 在colab上使用的云端数据\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "ABVc28LYyrFM",
        "outputId": "da52951e-6331-4876-cc1d-6f4642271630"
      },
      "source": [
        "import os\r\n",
        "path = \"/content/drive/My Drive/DeepLearning_AI_Course/Deeplizard/Keras - Python Deep Learning Neural Network API/Data/dogs-vs-cats\"\r\n",
        "\r\n",
        "# 更改当前系统的路径\r\n",
        "os.chdir(path)\r\n",
        "# 显示当前路径的文件夹名字\r\n",
        "print(os.listdir())\r\n",
        "\r\n",
        "# 获得当前路径\r\n",
        "os.getcwd()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Dog', 'Cat', 'train', 'valid', 'test']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/DeepLearning_AI_Course/Deeplizard/Keras - Python Deep Learning Neural Network API/Data/dogs-vs-cats'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juywmagqz9IP",
        "outputId": "a9c8c126-66fb-4af8-ec17-e0e4231c0084"
      },
      "source": [
        "os.chdir('..')\r\n",
        "print(os.listdir())\r\n",
        "train_path = './dogs-vs-cats/train'\r\n",
        "valid_path = './dogs-vs-cats/valid'\r\n",
        "test_path = './dogs-vs-cats/test'\r\n",
        "train_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input) \\\r\n",
        "    .flow_from_directory(directory=train_path, target_size=(224,224), classes=['cat', 'dog'], batch_size=10)\r\n",
        "valid_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input) \\\r\n",
        "    .flow_from_directory(directory=valid_path, target_size=(224,224), classes=['cat', 'dog'], batch_size=10)\r\n",
        "test_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input) \\\r\n",
        "    .flow_from_directory(directory=test_path, target_size=(224,224), classes=['cat', 'dog'], batch_size=10, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['dogs-vs-cats']\n",
            "Found 1000 images belonging to 2 classes.\n",
            "Found 200 images belonging to 2 classes.\n",
            "Found 100 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzI-LTDt1w9P"
      },
      "source": [
        "现在是时候训练模型了。\r\n",
        "\r\n",
        "我们已经引入了model.fit（）函数来训练上一集中的模型。 除了现在，我们将以相同的方式使用它，除了现在，我们将传入新引入的DirectoryIterators train_batches和valid_batches来训练和验证模型。 回想一下，这些是在上一集中创建的。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JhhLvmEO0g2X",
        "outputId": "121eb709-1d05-47ba-868a-d4982bc1a4bc"
      },
      "source": [
        "model.fit(x=train_batches,\r\n",
        "    steps_per_epoch=len(train_batches),\r\n",
        "    validation_data=valid_batches,\r\n",
        "    validation_steps=len(valid_batches),\r\n",
        "    epochs=10,\r\n",
        "    verbose=2\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "UnknownError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-c4ecaaffd38c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnknownError\u001b[0m: 2 root error(s) found.\n  (0) Unknown:  UnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x7fcf600a0ba0>\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/script_ops.py\", line 249, in __call__\n    ret = func(*args)\n\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py\", line 620, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 891, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\", line 807, in wrapped_generator\n    for data in generator_fn():\n\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\", line 933, in generator_fn\n    yield x[i]\n\n  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py\", line 65, in __getitem__\n    return self._get_batches_of_transformed_samples(index_array)\n\n  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py\", line 230, in _get_batches_of_transformed_samples\n    interpolation=self.interpolation)\n\n  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/utils.py\", line 114, in load_img\n    img = pil_image.open(io.BytesIO(f.read()))\n\n  File \"/usr/local/lib/python3.6/dist-packages/PIL/Image.py\", line 2862, in open\n    \"cannot identify image file %r\" % (filename if filename else fp)\n\nPIL.UnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x7fcf600a0ba0>\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n  (1) Unknown:  UnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x7fcf600a0ba0>\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/script_ops.py\", line 249, in __call__\n    ret = func(*args)\n\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py\", line 620, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 891, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\", line 807, in wrapped_generator\n    for data in generator_fn():\n\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\", line 933, in generator_fn\n    yield x[i]\n\n  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py\", line 65, in __getitem__\n    return self._get_batches_of_transformed_samples(index_array)\n\n  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py\", line 230, in _get_batches_of_transformed_samples\n    interpolation=self.interpolation)\n\n  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/utils.py\", line 114, in load_img\n    img = pil_image.open(io.BytesIO(f.read()))\n\n  File \"/usr/local/lib/python3.6/dist-packages/PIL/Image.py\", line 2862, in open\n    \"cannot identify image file %r\" % (filename if filename else fp)\n\nPIL.UnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x7fcf600a0ba0>\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n\t [[IteratorGetNext/_2]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_680]\n\nFunction call stack:\ntrain_function -> train_function\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MO8nC995JGJ"
      },
      "source": [
        "我们需要指定steps_per_epoch来指示在宣布一个时期完成之前，应将训练集中的多少批次样本传递给模型。 由于我们的训练集中有1000个样本，并且批次大小为10，所以我们将steps_per_epoch设置为100，因为100个批次（每10个样本）将涵盖我们的整个训练集。\r\n",
        "\r\n",
        "我们可以使用len（train_batches）作为指定此值的更通用的方法，因为train_batches的长度等于100，因为它由100个批次的10个样本组成。 同样，我们以相同的方式指定validate_steps，但使用validate_batches。\r\n",
        "\r\n",
        "我们指定10作为我们想要运行的epoch的数量，并将verbose参数设置为2，这只是指定在训练期间打印到控制台的日志输出的详细程度。\r\n",
        "\r\n",
        "从上面的输出中，我们可以看到这个简单模型在训练集上的性能很好，准确率达到100%，loss接近于0，但是，通过将这些结果与验证指标进行比较，我们可以看到我们的模型对训练数据有很大的过拟合。\r\n",
        "\r\n",
        "在这一点上，我们可以继续研究该模型以应对过度拟合，或者可以尝试另一种对数据使用预训练模型的方法。 我们将在下一集中探讨后者！\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4iCTQ4y3vbs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}