{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "11_Code Update For CNN Training With TensorFlow's Keras API.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPv+o0F/e9SX2+jnb2ETcdn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YinGuoX/Deep_Learning_Keras_WithDeeplizard/blob/master/11_Code_Update_For_CNN_Training_With_TensorFlow's_Keras_API.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yt-YYgf2kgsN"
      },
      "source": [
        "# Code Update For CNN Training With TensorFlow's Keras API\r\n",
        "在本集中，我们将讨论在构建和训练我们的第一个卷积神经网络(CNN)之前，我们需要知道关于即将到来的代码的更新。\r\n",
        "\r\n",
        "在即将到来的章节中，我们将演示如何使用我们以前组织和处理的图像数据训练CNN。\r\n",
        "\r\n",
        "回想一下，我们使用ImageDataGenerator.flow_from_directory()函数将图像数据存储在Keras序列中，特别是一个DirectoryIterator中。这个函数从磁盘上的指定位置生成成批的图像数据。\r\n",
        "\r\n",
        "正如您在上一节中所看到的，当我们训练一个模型时，我们在模型上调用fit()函数并传递训练数据。我们已经看到当我们的训练数据存储在一个简单的numpy数组中时是如何做到这一点的，但是在即将到来的CNN章节中，我们将看到如何专门为存储在DirectoryIterator中的训练数据做到这一点。\r\n",
        "\r\n",
        "最近，TensorFlow引入了一个变化，现在要求我们在数据存储在无限重复的数据集中(如DirectoryIterator)时，将另一个参数传递给fit()函数。\r\n",
        "\r\n",
        "\r\n",
        "**注意:**DirectoryIterator实际上是一个无限重复的数据集，因为迭代器生成的数据批将无限地持续出现，只要我们希望它们出现。您可以通过将迭代器传递给内置的Python next()函数并反复运行它来无限地生成新一批数据来看到这一点。\r\n",
        "\r\n",
        "## 1. 所需steps_per_epoch参数\r\n",
        "现在，回到需要为这种类型的数据传递给fit()函数的参数。这个参数被称为steps_per_epoch，也即声明在一个epoch结束并开始下一个epoch之前从训练集中产生的步骤数(样本批数)。\r\n",
        "\r\n",
        "这通常被设置为等于我们训练集中的样本数除以批大小。例如，如果我们有100个训练图像，我们的批处理大小是5，那么我们将设置steps_per_epoch=20。\r\n",
        "\r\n",
        "这个参数实际上并不是新的，但是，在以前的TensorFlow版本中，当我们的数据存储在Keras序列中时，不需要指定它，就像存储数据的DirectoryIterator那样。相反，TensorFlow会默认使用数据集的大小除以批大小作为steps_per_epoch的数量。\r\n",
        "\r\n",
        "取决于你运行的是哪个版本的TensorFlow，如果你没有指定这个参数，那么model.fit()将在第一个epoch上无限运行，永远不会完成。\r\n",
        "\r\n",
        "## 2. 一些其他的参数\r\n",
        "请注意，除了我们在调用model.fit()时指定的steps_per_epoch，如果我们还将验证数据传递给模型的话，我们还需要指定一个名为validation_steps的参数。这个参数的作用方式与steps_per_epoch完全相同，只是在验证集上而已。\r\n",
        "\r\n",
        "最后，当我们通过在模型上调用predict()并传入测试集来使用模型进行推理时，我们还需要在这里指定称为steps的参数。在本例中，这是声明预测轮完成之前从测试集中产生的步骤(样本批次)的数量。\r\n",
        "\r\n",
        "## 3.跟踪该问题\r\n",
        "目前还不清楚是否会继续需要这个参数，因为这种无限重复的数据步骤最初需要它，然后不需要，现在又需要。如果你感兴趣，你可以在[TensorFlow Github](https://github.com/tensorflow/tensorflow/issues/39277)上追踪这个问题。\r\n",
        "\r\n",
        "在接下来的章节中，当我们调用fit()或predict()时，您将看到在章节的视频部分，steps_per_epoch、validation_steps和steps没有指定。然而，相应的博客会更新TensorFlow当时要求的代码。因此，如果需要这些参数，它们将在博客中设置。如果不再需要这些参数，则不会在blog中设置它们。\r\n",
        "\r\n",
        "现在我们准备在下一集开始构建和训练我们的第一个卷积神经网络。\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lK3wgkIvkeh1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}