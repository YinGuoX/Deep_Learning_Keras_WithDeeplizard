{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5_Train An Artificial Neural Network With TensorFlow's Keras API.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPrrxj9FxKN4nAEWwqAFs4p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YinGuoX/Deep_Learning_Keras_WithDeeplizard/blob/master/5_Train_An_Artificial_Neural_Network_With_TensorFlow's_Keras_API.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvuHfX-QDo7E"
      },
      "source": [
        "# Train An Artificial Neural Network With TensorFlow's Keras API\r\n",
        "在本集中，我们将演示如何使用TensorFlow集成的Keras API训练人工神经网络。在上一集中，我们经历了构建简单网络的步骤，现在我们将重点关注使用我们在更早的一集中生成的数据来训练它。\r\n",
        "\r\n",
        "部署上一节中的神经网络模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6E8sruCDgVg"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "from tensorflow import keras\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras.layers import Activation, Dense\r\n",
        "from tensorflow.keras.optimizers import Adam\r\n",
        "from tensorflow.keras.metrics import categorical_crossentropy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6CaZCzVD4Xf"
      },
      "source": [
        "model = Sequential([\r\n",
        "    Dense(units=16, input_shape=(1,), activation='relu'),\r\n",
        "    Dense(units=32, activation='relu'),\r\n",
        "    Dense(units=2, activation='softmax')\r\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87yNqE8CD61c"
      },
      "source": [
        "## 1. 编译模型\r\n",
        "为了使模型准备好进行训练，我们需要做的第一件事是在其上调用compile（）函数。\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjZpsVhLD5jS"
      },
      "source": [
        "model.compile(optimizer=Adam(learning_rate=0.0001),\r\n",
        "              loss='sparse_categorical_crossentropy',\r\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUEPzr6dET4C"
      },
      "source": [
        "这个函数为训练配置模型，并需要一些参数。首先，我们指定优化器Adam。Adam接受了一个可选参数学习率，我们将其设置为0.0001。（Adam optimization是一种随机梯度下降(SGD)方法）\r\n",
        "\r\n",
        "我们指定的下一个参数是loss。我们将使用sparse_categorical_crossentropy，因为我们的标签是整数格式的。\r\n",
        "\r\n",
        "**注意：**当我们只有两个类时，我们可以将输出层配置为仅一个输出，而不是两个输出，并使用binary_crossentropy作为损失，而不是categorical_crossentropy。 两种选择均能很好地工作，并获得完全相同的结果，但是，对于binary_crossentropy，最后一层需要使用Sigmoid而不是softmax作为其激活函数。\r\n",
        "\r\n",
        "在compile()中指定的最后一个参数是metrics。这个参数期望一个我们希望模型在培训和测试期间评估的指标列表。我们将它设置为一个包含字符串accuracy的列表。\r\n",
        "\r\n",
        "## 2. 训练模型\r\n",
        "现在模型已经编译完成，我们可以使用fit()函数来训练它。\r\n",
        "\r\n",
        "准备数据集"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BLYJEZsFeyV",
        "outputId": "c78f267e-a5f5-451e-a026-8a8e7f25909a"
      },
      "source": [
        "import numpy as np\r\n",
        "from random import randint\r\n",
        "from sklearn.utils import shuffle\r\n",
        "from sklearn.preprocessing import MinMaxScaler\r\n",
        "\r\n",
        "train_labels = []\r\n",
        "train_samples = []\r\n",
        "\r\n",
        "# 生成数据\r\n",
        "for i in range(50):\r\n",
        "    # 大约5%的年轻人确实经历过副作用\r\n",
        "    random_younger = randint(13,64)\r\n",
        "    train_samples.append(random_younger)\r\n",
        "    train_labels.append(1)\r\n",
        "\r\n",
        "    # 大约5%的老年人没有经历过副作用\r\n",
        "    random_older = randint(65,100)\r\n",
        "    train_samples.append(random_older)\r\n",
        "    train_labels.append(0)\r\n",
        "\r\n",
        "for i in range(1000):\r\n",
        "    # 大约95%的年轻人没有经历过副作用\r\n",
        "    random_younger = randint(13,64)\r\n",
        "    train_samples.append(random_younger)\r\n",
        "    train_labels.append(0)\r\n",
        "\r\n",
        "    # 大约95%的老年人确实经历过副作用\r\n",
        "    random_older = randint(65,100)\r\n",
        "    train_samples.append(random_older)\r\n",
        "    train_labels.append(1)\r\n",
        "\r\n",
        "train_labels = np.array(train_labels)\r\n",
        "train_samples = np.array(train_samples)\r\n",
        "train_labels,train_samples = shuffle(train_labels,train_samples)\r\n",
        "print(train_samples.shape)\r\n",
        "\r\n",
        "# 通过将每个特征缩放到给定的范围来转换特征。\r\n",
        "scaler = MinMaxScaler(feature_range=(0,1))\r\n",
        "\r\n",
        "scaled_train_samples = scaler.fit_transform(train_samples.reshape(-1,1))\r\n",
        "# 我们只是根据fit_transform（）函数默认情况下不接受一维数据的情况，将数据重塑为2D。\r\n",
        "print(train_samples.reshape(-1,1).shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2100,)\n",
            "(2100, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8D98emCETQG",
        "outputId": "56e71058-b742-4c2e-9d0c-8465ea4b0331"
      },
      "source": [
        "model.fit(x=scaled_train_samples,y=train_labels,batch_size=10,epochs=30,verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "210/210 - 1s - loss: 0.6954 - accuracy: 0.5267\n",
            "Epoch 2/30\n",
            "210/210 - 0s - loss: 0.6564 - accuracy: 0.6881\n",
            "Epoch 3/30\n",
            "210/210 - 0s - loss: 0.6185 - accuracy: 0.7362\n",
            "Epoch 4/30\n",
            "210/210 - 0s - loss: 0.5836 - accuracy: 0.7795\n",
            "Epoch 5/30\n",
            "210/210 - 0s - loss: 0.5481 - accuracy: 0.7995\n",
            "Epoch 6/30\n",
            "210/210 - 0s - loss: 0.5134 - accuracy: 0.8371\n",
            "Epoch 7/30\n",
            "210/210 - 0s - loss: 0.4792 - accuracy: 0.8524\n",
            "Epoch 8/30\n",
            "210/210 - 0s - loss: 0.4479 - accuracy: 0.8700\n",
            "Epoch 9/30\n",
            "210/210 - 0s - loss: 0.4207 - accuracy: 0.8790\n",
            "Epoch 10/30\n",
            "210/210 - 0s - loss: 0.3970 - accuracy: 0.8929\n",
            "Epoch 11/30\n",
            "210/210 - 0s - loss: 0.3770 - accuracy: 0.8929\n",
            "Epoch 12/30\n",
            "210/210 - 0s - loss: 0.3601 - accuracy: 0.9000\n",
            "Epoch 13/30\n",
            "210/210 - 0s - loss: 0.3461 - accuracy: 0.9081\n",
            "Epoch 14/30\n",
            "210/210 - 0s - loss: 0.3345 - accuracy: 0.9090\n",
            "Epoch 15/30\n",
            "210/210 - 0s - loss: 0.3248 - accuracy: 0.9124\n",
            "Epoch 16/30\n",
            "210/210 - 0s - loss: 0.3167 - accuracy: 0.9186\n",
            "Epoch 17/30\n",
            "210/210 - 0s - loss: 0.3099 - accuracy: 0.9152\n",
            "Epoch 18/30\n",
            "210/210 - 0s - loss: 0.3041 - accuracy: 0.9210\n",
            "Epoch 19/30\n",
            "210/210 - 0s - loss: 0.2994 - accuracy: 0.9181\n",
            "Epoch 20/30\n",
            "210/210 - 0s - loss: 0.2953 - accuracy: 0.9238\n",
            "Epoch 21/30\n",
            "210/210 - 0s - loss: 0.2918 - accuracy: 0.9214\n",
            "Epoch 22/30\n",
            "210/210 - 0s - loss: 0.2887 - accuracy: 0.9286\n",
            "Epoch 23/30\n",
            "210/210 - 0s - loss: 0.2862 - accuracy: 0.9248\n",
            "Epoch 24/30\n",
            "210/210 - 0s - loss: 0.2839 - accuracy: 0.9243\n",
            "Epoch 25/30\n",
            "210/210 - 0s - loss: 0.2818 - accuracy: 0.9286\n",
            "Epoch 26/30\n",
            "210/210 - 0s - loss: 0.2799 - accuracy: 0.9314\n",
            "Epoch 27/30\n",
            "210/210 - 0s - loss: 0.2783 - accuracy: 0.9310\n",
            "Epoch 28/30\n",
            "210/210 - 0s - loss: 0.2770 - accuracy: 0.9314\n",
            "Epoch 29/30\n",
            "210/210 - 0s - loss: 0.2753 - accuracy: 0.9290\n",
            "Epoch 30/30\n",
            "210/210 - 0s - loss: 0.2739 - accuracy: 0.9329\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fe533bc6710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNelx04mF3J5"
      },
      "source": [
        "我们传递给fit()函数的第一项是训练集x。回想一下之前的章节，我们创建了训练集并将其命名为scaled_train_samples.\r\n",
        "\r\n",
        "我们设置的下一个参数是训练集y的标签，我们先前将其命名为train_labels。\r\n",
        "\r\n",
        "然后，我们指定batch_size。 同样，“深度学习基础知识”课程详细介绍了批量大小的概念。\r\n",
        "\r\n",
        "接下来，我们指定要运行多少个epoch。 我们将其设置为30。请注意，一个时期是所有数据到网络的单次传递。\r\n",
        "\r\n",
        "**注意：**\r\n",
        "* 一个epoch是所有数据都经过了一次神经网络模型的训练\r\n",
        "* batch_size=10:以每次10张图片为一批进入神经网络模型进行训练\r\n",
        "* 因此一个epoch要training set size / batch_size个批次完成一个所有数据的训练\r\n",
        "\r\n",
        "\r\n",
        "最后，我们指定verbose = 2。 这只是指定我们希望在每个培训时期看到多少输出到控制台。 详细程度范围为0到2，因此我们得到的是最详细的输出。\r\n",
        "\r\n",
        "我们可以看到这30个时期的对应输出。通过损失和准确性判断，我们可以看到这两个指标随着时间的推移稳步提高，精确度达到93%，损失稳步减少，直到我们达到0.27。\r\n",
        "\r\n",
        "请注意，尽管这是一个在简单数据上训练的非常简单的模型，但无需付出太多努力，但我们能够以相对较快的时间达到很好的结果。 在随后的几集中，我们将演示更复杂的模型以及更复杂的数据，但是希望您对我们能够轻松开始使用tf.keras感到鼓舞。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lB4k9mA1FK_N"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}