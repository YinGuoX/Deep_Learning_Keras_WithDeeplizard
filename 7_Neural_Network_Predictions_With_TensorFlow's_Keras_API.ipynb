{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "7_Neural Network Predictions With TensorFlow's Keras API.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOXbI5ruBweVxz9ipONvQgj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YinGuoX/Deep_Learning_Keras_WithDeeplizard/blob/master/7_Neural_Network_Predictions_With_TensorFlow's_Keras_API.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsEFr18-OVCn"
      },
      "source": [
        "# Neural Network Predictions With TensorFlow's Keras API\r\n",
        "\r\n",
        "在本集中，我们将演示如何使用神经网络进行推理以对测试集中的数据进行预测。 我们将继续使用与前几集相同的tf.keras.Sequential模型和数据。\r\n",
        "\r\n",
        "正如我们之前提到的那样，当我们训练模型时，希望我们以后能够采用训练后的模型，将其应用于新数据，并使模型进行概括并准确地预测之前从未见过的数据 。\r\n",
        "\r\n",
        "例如，假设我们有一个对猫或狗的图像进行分类的模型，并且训练数据包含来自特定在线数据集的数千个猫和狗的图像。\r\n",
        "\r\n",
        "## 1. 什么是推理\r\n",
        "\r\n",
        "现在假设以后我们要采用该模型并将其用于从不同数据集中预测猫和狗的其他图像。 希望是，即使我们的模型在训练过程中没有训练过这些特定的猫狗图像中，它仍然能够基于从猫和狗数据集中学到的信息为它们准确地做出预测。\r\n",
        "\r\n",
        "我们称这个过程为推理，因为模型使用它从训练中获得的知识，并使用它来推断预测或结果。\r\n",
        "\r\n",
        "至此，我们在过去几集中一直在使用的模型已经过培训和验证。 鉴于我们从验证数据中看到的结果，该模型似乎可以很好地预测新的测试集。\r\n",
        "\r\n",
        "注意，测试集是训练结束后专门用于推理的数据集。 您可以在“深度学习基础知识”课程中找到有关深度学习中使用的所有不同类型数据集的更多信息。\r\n",
        "\r\n",
        "## 2. 创建测试集\r\n",
        "我们将以创建训练集的相同方式创建测试集。 通常，应始终以与训练集相同的方式处理测试集。\r\n",
        "\r\n",
        "我们不会逐步介绍下面用于生成和处理测试数据的代码，因为在生成训练数据的前一集中已经详细介绍了该代码，但是请确保您已将所有导入内容都包含在其中。 从之前的情节开始到现在为止，以及所有现有的代码。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XqwdrKC2QBFR",
        "outputId": "1f69a116-0e46-456a-8d23-8f70a47f45ce"
      },
      "source": [
        "# 准备数据\r\n",
        "import numpy as np\r\n",
        "from random import randint\r\n",
        "from sklearn.utils import shuffle\r\n",
        "from sklearn.preprocessing import MinMaxScaler\r\n",
        "\r\n",
        "train_labels = []\r\n",
        "train_samples = []\r\n",
        "\r\n",
        "# 生成数据\r\n",
        "for i in range(50):\r\n",
        "    # 大约5%的年轻人确实经历过副作用\r\n",
        "    random_younger = randint(13,64)\r\n",
        "    train_samples.append(random_younger)\r\n",
        "    train_labels.append(1)\r\n",
        "\r\n",
        "    # 大约5%的老年人没有经历过副作用\r\n",
        "    random_older = randint(65,100)\r\n",
        "    train_samples.append(random_older)\r\n",
        "    train_labels.append(0)\r\n",
        "\r\n",
        "for i in range(1000):\r\n",
        "    # 大约95%的年轻人没有经历过副作用\r\n",
        "    random_younger = randint(13,64)\r\n",
        "    train_samples.append(random_younger)\r\n",
        "    train_labels.append(0)\r\n",
        "\r\n",
        "    # 大约95%的老年人确实经历过副作用\r\n",
        "    random_older = randint(65,100)\r\n",
        "    train_samples.append(random_older)\r\n",
        "    train_labels.append(1)\r\n",
        "\r\n",
        "train_labels = np.array(train_labels)\r\n",
        "train_samples = np.array(train_samples)\r\n",
        "train_labels,train_samples = shuffle(train_labels,train_samples)\r\n",
        "print(train_samples.shape)\r\n",
        "\r\n",
        "# 通过将每个特征缩放到给定的范围来转换特征。\r\n",
        "scaler = MinMaxScaler(feature_range=(0,1))\r\n",
        "\r\n",
        "scaled_train_samples = scaler.fit_transform(train_samples.reshape(-1,1))\r\n",
        "# 我们只是根据fit_transform（）函数默认情况下不接受一维数据的情况，将数据重塑为2D。\r\n",
        "print(train_samples.reshape(-1,1).shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2100,)\n",
            "(2100, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Icko-ba2OSsE"
      },
      "source": [
        "\r\n",
        "\r\n",
        "test_labels =  []\r\n",
        "\r\n",
        "test_samples = []\r\n",
        "\r\n",
        "for i in range(10):\r\n",
        "    # The 5% of younger individuals who did experience side effects\r\n",
        "    random_younger = randint(13,64)\r\n",
        "    test_samples.append(random_younger)\r\n",
        "    test_labels.append(1)\r\n",
        "\r\n",
        "    # The 5% of older individuals who did not experience side effects\r\n",
        "    random_older = randint(65,100)\r\n",
        "    test_samples.append(random_older)\r\n",
        "    test_labels.append(0)\r\n",
        "\r\n",
        "for i in range(200):\r\n",
        "    # The 95% of younger individuals who did not experience side effects\r\n",
        "    random_younger = randint(13,64)\r\n",
        "    test_samples.append(random_younger)\r\n",
        "    test_labels.append(0)\r\n",
        "\r\n",
        "    # The 95% of older individuals who did experience side effects\r\n",
        "    random_older = randint(65,100)\r\n",
        "    test_samples.append(random_older)\r\n",
        "    test_labels.append(1)\r\n",
        "\r\n",
        "test_labels = np.array(test_labels)\r\n",
        "test_samples = np.array(test_samples)\r\n",
        "test_labels, test_samples = shuffle(test_labels, test_samples)\r\n",
        "\r\n",
        "scaled_test_samples = scaler.fit_transform(test_samples.reshape(-1,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9XyNCjJQTl2"
      },
      "source": [
        "# 创建并且编译模型\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow import keras\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras.layers import Activation, Dense\r\n",
        "from tensorflow.keras.optimizers import Adam\r\n",
        "from tensorflow.keras.metrics import categorical_crossentropy\r\n",
        "\r\n",
        "model = Sequential([\r\n",
        "    Dense(units=16, input_shape=(1,), activation='relu'),\r\n",
        "    Dense(units=32, activation='relu'),\r\n",
        "    Dense(units=2, activation='softmax')\r\n",
        "])\r\n",
        "\r\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001),\r\n",
        "              loss='sparse_categorical_crossentropy',\r\n",
        "              metrics=['accuracy'])\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WY_PaxA-QWC4",
        "outputId": "7c6347da-11bc-404d-d15a-4e69d92d0ac4"
      },
      "source": [
        "# 训练模型\r\n",
        "model.fit(\r\n",
        "    x = scaled_train_samples,\r\n",
        "    y = train_labels,\r\n",
        "    validation_split=0.1,\r\n",
        "    batch_size = 10,\r\n",
        "    epochs = 30,\r\n",
        "    verbose=2\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "189/189 - 1s - loss: 0.7029 - accuracy: 0.4074 - val_loss: 0.6829 - val_accuracy: 0.4714\n",
            "Epoch 2/30\n",
            "189/189 - 0s - loss: 0.6623 - accuracy: 0.5593 - val_loss: 0.6581 - val_accuracy: 0.5238\n",
            "Epoch 3/30\n",
            "189/189 - 0s - loss: 0.6341 - accuracy: 0.6021 - val_loss: 0.6375 - val_accuracy: 0.5762\n",
            "Epoch 4/30\n",
            "189/189 - 0s - loss: 0.6093 - accuracy: 0.6434 - val_loss: 0.6169 - val_accuracy: 0.6238\n",
            "Epoch 5/30\n",
            "189/189 - 0s - loss: 0.5854 - accuracy: 0.6725 - val_loss: 0.5965 - val_accuracy: 0.6571\n",
            "Epoch 6/30\n",
            "189/189 - 0s - loss: 0.5639 - accuracy: 0.7074 - val_loss: 0.5752 - val_accuracy: 0.7000\n",
            "Epoch 7/30\n",
            "189/189 - 0s - loss: 0.5405 - accuracy: 0.7481 - val_loss: 0.5525 - val_accuracy: 0.7286\n",
            "Epoch 8/30\n",
            "189/189 - 0s - loss: 0.5175 - accuracy: 0.7778 - val_loss: 0.5303 - val_accuracy: 0.7714\n",
            "Epoch 9/30\n",
            "189/189 - 0s - loss: 0.4953 - accuracy: 0.7995 - val_loss: 0.5096 - val_accuracy: 0.7762\n",
            "Epoch 10/30\n",
            "189/189 - 0s - loss: 0.4738 - accuracy: 0.8122 - val_loss: 0.4884 - val_accuracy: 0.8429\n",
            "Epoch 11/30\n",
            "189/189 - 0s - loss: 0.4531 - accuracy: 0.8365 - val_loss: 0.4689 - val_accuracy: 0.8619\n",
            "Epoch 12/30\n",
            "189/189 - 0s - loss: 0.4332 - accuracy: 0.8450 - val_loss: 0.4497 - val_accuracy: 0.8714\n",
            "Epoch 13/30\n",
            "189/189 - 0s - loss: 0.4144 - accuracy: 0.8587 - val_loss: 0.4316 - val_accuracy: 0.8810\n",
            "Epoch 14/30\n",
            "189/189 - 0s - loss: 0.3969 - accuracy: 0.8693 - val_loss: 0.4153 - val_accuracy: 0.8905\n",
            "Epoch 15/30\n",
            "189/189 - 0s - loss: 0.3803 - accuracy: 0.8788 - val_loss: 0.3996 - val_accuracy: 0.8905\n",
            "Epoch 16/30\n",
            "189/189 - 0s - loss: 0.3651 - accuracy: 0.8931 - val_loss: 0.3854 - val_accuracy: 0.8952\n",
            "Epoch 17/30\n",
            "189/189 - 0s - loss: 0.3510 - accuracy: 0.8958 - val_loss: 0.3723 - val_accuracy: 0.8952\n",
            "Epoch 18/30\n",
            "189/189 - 0s - loss: 0.3382 - accuracy: 0.9042 - val_loss: 0.3605 - val_accuracy: 0.8952\n",
            "Epoch 19/30\n",
            "189/189 - 0s - loss: 0.3269 - accuracy: 0.9106 - val_loss: 0.3504 - val_accuracy: 0.8952\n",
            "Epoch 20/30\n",
            "189/189 - 0s - loss: 0.3166 - accuracy: 0.9196 - val_loss: 0.3415 - val_accuracy: 0.8952\n",
            "Epoch 21/30\n",
            "189/189 - 0s - loss: 0.3076 - accuracy: 0.9196 - val_loss: 0.3334 - val_accuracy: 0.9048\n",
            "Epoch 22/30\n",
            "189/189 - 0s - loss: 0.2996 - accuracy: 0.9233 - val_loss: 0.3268 - val_accuracy: 0.9048\n",
            "Epoch 23/30\n",
            "189/189 - 0s - loss: 0.2926 - accuracy: 0.9238 - val_loss: 0.3209 - val_accuracy: 0.9048\n",
            "Epoch 24/30\n",
            "189/189 - 0s - loss: 0.2866 - accuracy: 0.9265 - val_loss: 0.3158 - val_accuracy: 0.9095\n",
            "Epoch 25/30\n",
            "189/189 - 0s - loss: 0.2810 - accuracy: 0.9280 - val_loss: 0.3110 - val_accuracy: 0.9095\n",
            "Epoch 26/30\n",
            "189/189 - 0s - loss: 0.2763 - accuracy: 0.9312 - val_loss: 0.3069 - val_accuracy: 0.9286\n",
            "Epoch 27/30\n",
            "189/189 - 0s - loss: 0.2722 - accuracy: 0.9328 - val_loss: 0.3038 - val_accuracy: 0.9286\n",
            "Epoch 28/30\n",
            "189/189 - 0s - loss: 0.2686 - accuracy: 0.9339 - val_loss: 0.3012 - val_accuracy: 0.9286\n",
            "Epoch 29/30\n",
            "189/189 - 0s - loss: 0.2654 - accuracy: 0.9354 - val_loss: 0.2988 - val_accuracy: 0.9286\n",
            "Epoch 30/30\n",
            "189/189 - 0s - loss: 0.2626 - accuracy: 0.9349 - val_loss: 0.2966 - val_accuracy: 0.9286\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f86e7b695c0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KCZsY6aQKt8"
      },
      "source": [
        "## 3. 评估测试集\r\n",
        "为了从测试集的模型中获得预测，我们调用model.predict（）。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vVDUusoQFe2"
      },
      "source": [
        "predictions = model.predict(\r\n",
        "    x=scaled_test_samples,\r\n",
        "    batch_size = 10,\r\n",
        "    verbose=0\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f41WF_ceQtiH"
      },
      "source": [
        "对于predict()函数，我们传递测试样本x，指定batch_size，并指定在预测生成期间希望从日志消息中获取的详细级别。 由于预测的输出与我们无关，因此我们将verbose = 0设置为无输出。\r\n",
        "\r\n",
        "**注意：**与训练和验证集不同，我们在推断阶段不会将测试集的标签传递给模型。\r\n",
        "\r\n",
        "要查看模型的预测是什么样子的，我们可以遍历它们并打印出来。\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B74AxEudQjEa",
        "outputId": "4c88db66-3a80-461e-915c-852d743291c0"
      },
      "source": [
        "for i in predictions:\r\n",
        "  print(i)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.04284908 0.957151  ]\n",
            "[0.09522419 0.9047758 ]\n",
            "[0.10627808 0.89372194]\n",
            "[0.91552067 0.08447936]\n",
            "[0.17964612 0.82035387]\n",
            "[0.9194009  0.08059911]\n",
            "[0.21848594 0.7815141 ]\n",
            "[0.90894824 0.09105177]\n",
            "[0.01419716 0.98580277]\n",
            "[0.01743693 0.98256314]\n",
            "[0.01419716 0.98580277]\n",
            "[0.01937912 0.9806209 ]\n",
            "[0.9194009  0.08059911]\n",
            "[0.06065821 0.93934184]\n",
            "[0.92383343 0.07616653]\n",
            "[0.86882836 0.13117167]\n",
            "[0.24005216 0.75994784]\n",
            "[0.86882836 0.13117167]\n",
            "[0.01937912 0.9806209 ]\n",
            "[0.88110024 0.11889981]\n",
            "[0.9252606  0.07473939]\n",
            "[0.0301011 0.9698989]\n",
            "[0.02414715 0.9758529 ]\n",
            "[0.78414047 0.21585952]\n",
            "[0.0301011 0.9698989]\n",
            "[0.4261517  0.57384837]\n",
            "[0.01743693 0.98256314]\n",
            "[0.9209039  0.07909605]\n",
            "[0.0540622 0.9459379]\n",
            "[0.02414715 0.9758529 ]\n",
            "[0.92735523 0.07264473]\n",
            "[0.04284908 0.957151  ]\n",
            "[0.9225497  0.07745028]\n",
            "[0.03811106 0.9618889 ]\n",
            "[0.14641725 0.85358274]\n",
            "[0.9195102  0.08048978]\n",
            "[0.928792 0.071208]\n",
            "[0.4562491  0.54375094]\n",
            "[0.02163504 0.978365  ]\n",
            "[0.90894824 0.09105177]\n",
            "[0.9194009  0.08059911]\n",
            "[0.08521038 0.9147896 ]\n",
            "[0.60709953 0.39290047]\n",
            "[0.48666963 0.5133304 ]\n",
            "[0.36776778 0.63223225]\n",
            "[0.0128075 0.9871925]\n",
            "[0.13180414 0.86819583]\n",
            "[0.28737688 0.7126232 ]\n",
            "[0.9277168  0.07228328]\n",
            "[0.9147192  0.08528076]\n",
            "[0.88110024 0.11889981]\n",
            "[0.0338784 0.9661216]\n",
            "[0.9231106  0.07688943]\n",
            "[0.9245501  0.07544986]\n",
            "[0.36776778 0.63223225]\n",
            "[0.0301011 0.9698989]\n",
            "[0.21848594 0.7815141 ]\n",
            "[0.8040935  0.19590652]\n",
            "[0.9277168  0.07228328]\n",
            "[0.78414047 0.21585952]\n",
            "[0.04284908 0.957151  ]\n",
            "[0.8923662  0.10763384]\n",
            "[0.01937912 0.9806209 ]\n",
            "[0.04814665 0.95185333]\n",
            "[0.8226193  0.17738065]\n",
            "[0.9277168  0.07228328]\n",
            "[0.14641725 0.85358274]\n",
            "[0.4261517  0.57384837]\n",
            "[0.92015564 0.07984435]\n",
            "[0.9122735  0.08772642]\n",
            "[0.8397424  0.16025752]\n",
            "[0.8554976 0.1445024]\n",
            "[0.39659154 0.6034085 ]\n",
            "[0.04284908 0.957151  ]\n",
            "[0.9195102  0.08048978]\n",
            "[0.01573521 0.98426473]\n",
            "[0.9163152  0.08368481]\n",
            "[0.8226193  0.17738065]\n",
            "[0.8554976 0.1445024]\n",
            "[0.24005216 0.75994784]\n",
            "[0.88110024 0.11889981]\n",
            "[0.36776778 0.63223225]\n",
            "[0.16234756 0.8376525 ]\n",
            "[0.913911   0.08608903]\n",
            "[0.9280414  0.07195864]\n",
            "[0.8554976 0.1445024]\n",
            "[0.0301011 0.9698989]\n",
            "[0.01155225 0.9884477 ]\n",
            "[0.13180414 0.86819583]\n",
            "[0.690293   0.30970693]\n",
            "[0.9254179 0.0745821]\n",
            "[0.17964612 0.82035387]\n",
            "[0.07616099 0.92383903]\n",
            "[0.9114444  0.08855565]\n",
            "[0.01743693 0.98256314]\n",
            "[0.0540622 0.9459379]\n",
            "[0.9231106  0.07688943]\n",
            "[0.48666963 0.5133304 ]\n",
            "[0.01743693 0.98256314]\n",
            "[0.9231106  0.07688943]\n",
            "[0.0128075 0.9871925]\n",
            "[0.01743693 0.98256314]\n",
            "[0.8554976 0.1445024]\n",
            "[0.24005216 0.75994784]\n",
            "[0.57762194 0.422378  ]\n",
            "[0.0128075 0.9871925]\n",
            "[0.36776778 0.63223225]\n",
            "[0.51718897 0.48281103]\n",
            "[0.9223814  0.07761861]\n",
            "[0.16234756 0.8376525 ]\n",
            "[0.24005216 0.75994784]\n",
            "[0.8923662  0.10763384]\n",
            "[0.03811106 0.9618889 ]\n",
            "[0.14641725 0.85358274]\n",
            "[0.91787183 0.0821282 ]\n",
            "[0.02163504 0.978365  ]\n",
            "[0.0338784 0.9661216]\n",
            "[0.24005216 0.75994784]\n",
            "[0.0540622 0.9459379]\n",
            "[0.9223814  0.07761861]\n",
            "[0.913911   0.08608903]\n",
            "[0.690293   0.30970693]\n",
            "[0.14641725 0.85358274]\n",
            "[0.57762194 0.422378  ]\n",
            "[0.71577656 0.28422344]\n",
            "[0.48666963 0.5133304 ]\n",
            "[0.9225497  0.07745028]\n",
            "[0.0128075 0.9871925]\n",
            "[0.24005216 0.75994784]\n",
            "[0.51718897 0.48281103]\n",
            "[0.9225497  0.07745028]\n",
            "[0.04284908 0.957151  ]\n",
            "[0.01573521 0.98426473]\n",
            "[0.28737688 0.7126232 ]\n",
            "[0.91552067 0.08447936]\n",
            "[0.57762194 0.422378  ]\n",
            "[0.63581705 0.36418295]\n",
            "[0.9186396  0.08136038]\n",
            "[0.9223814  0.07761861]\n",
            "[0.06800118 0.9319988 ]\n",
            "[0.9195102  0.08048978]\n",
            "[0.01937912 0.9806209 ]\n",
            "[0.90894824 0.09105177]\n",
            "[0.01155225 0.9884477 ]\n",
            "[0.17964612 0.82035387]\n",
            "[0.21848594 0.7815141 ]\n",
            "[0.09522419 0.9047758 ]\n",
            "[0.1983516 0.8016484]\n",
            "[0.88110024 0.11889981]\n",
            "[0.9287215  0.07127851]\n",
            "[0.60709953 0.39290047]\n",
            "[0.9287215  0.07127851]\n",
            "[0.71577656 0.28422344]\n",
            "[0.28737688 0.7126232 ]\n",
            "[0.24005216 0.75994784]\n",
            "[0.9114444  0.08855565]\n",
            "[0.0128075 0.9871925]\n",
            "[0.92383343 0.07616653]\n",
            "[0.9280414  0.07195863]\n",
            "[0.0540622 0.9459379]\n",
            "[0.8554976 0.1445024]\n",
            "[0.9280414  0.07195864]\n",
            "[0.02414715 0.9758529 ]\n",
            "[0.01937912 0.9806209 ]\n",
            "[0.76275486 0.23724514]\n",
            "[0.9216459  0.07835415]\n",
            "[0.9194009  0.08059911]\n",
            "[0.9277168  0.07228328]\n",
            "[0.36776778 0.63223225]\n",
            "[0.7399531  0.26004687]\n",
            "[0.24005216 0.75994784]\n",
            "[0.92383343 0.07616653]\n",
            "[0.9280414  0.07195864]\n",
            "[0.8226193  0.17738065]\n",
            "[0.06800118 0.9319988 ]\n",
            "[0.09522419 0.9047758 ]\n",
            "[0.31301937 0.68698066]\n",
            "[0.04284908 0.957151  ]\n",
            "[0.39659154 0.6034085 ]\n",
            "[0.9194009  0.08059911]\n",
            "[0.4562491  0.54375094]\n",
            "[0.88110024 0.11889981]\n",
            "[0.88110024 0.11889981]\n",
            "[0.01573521 0.98426473]\n",
            "[0.63581705 0.36418295]\n",
            "[0.31301937 0.68698066]\n",
            "[0.1983516 0.8016484]\n",
            "[0.8397424  0.16025752]\n",
            "[0.0338784 0.9661216]\n",
            "[0.9225497  0.07745028]\n",
            "[0.8397424  0.16025752]\n",
            "[0.28737688 0.7126232 ]\n",
            "[0.01419716 0.98580277]\n",
            "[0.92596495 0.07403506]\n",
            "[0.01155225 0.9884477 ]\n",
            "[0.9147192  0.08528076]\n",
            "[0.24005216 0.75994784]\n",
            "[0.01155225 0.9884477 ]\n",
            "[0.6635984  0.33640158]\n",
            "[0.01155225 0.9884477 ]\n",
            "[0.10627808 0.89372194]\n",
            "[0.02694288 0.97305715]\n",
            "[0.92735523 0.07264473]\n",
            "[0.4562491  0.54375094]\n",
            "[0.690293   0.30970693]\n",
            "[0.92735523 0.07264473]\n",
            "[0.14641725 0.85358274]\n",
            "[0.4261517  0.57384837]\n",
            "[0.9280414  0.07195863]\n",
            "[0.4562491  0.54375094]\n",
            "[0.63581705 0.36418295]\n",
            "[0.36776778 0.63223225]\n",
            "[0.9114444  0.08855565]\n",
            "[0.06065821 0.93934184]\n",
            "[0.9254179 0.0745821]\n",
            "[0.90160155 0.09839846]\n",
            "[0.8923662  0.10763384]\n",
            "[0.24005216 0.75994784]\n",
            "[0.09522419 0.9047758 ]\n",
            "[0.01937912 0.9806209 ]\n",
            "[0.08521038 0.9147896 ]\n",
            "[0.92735523 0.07264473]\n",
            "[0.92596495 0.07403506]\n",
            "[0.13180414 0.86819583]\n",
            "[0.88110024 0.11889981]\n",
            "[0.13180414 0.86819583]\n",
            "[0.04814665 0.95185333]\n",
            "[0.36776778 0.63223225]\n",
            "[0.78414047 0.21585952]\n",
            "[0.17964612 0.82035387]\n",
            "[0.1184472 0.8815528]\n",
            "[0.01937912 0.9806209 ]\n",
            "[0.16234756 0.8376525 ]\n",
            "[0.7399531  0.26004687]\n",
            "[0.17964612 0.82035387]\n",
            "[0.92015564 0.07984435]\n",
            "[0.21848594 0.7815141 ]\n",
            "[0.31301937 0.68698066]\n",
            "[0.9194009  0.08059911]\n",
            "[0.9114444  0.08855565]\n",
            "[0.02163504 0.978365  ]\n",
            "[0.928792 0.071208]\n",
            "[0.14641725 0.85358274]\n",
            "[0.9170974  0.08290263]\n",
            "[0.90894824 0.09105177]\n",
            "[0.51718897 0.48281103]\n",
            "[0.02163504 0.978365  ]\n",
            "[0.92383343 0.07616653]\n",
            "[0.36776778 0.63223225]\n",
            "[0.01573521 0.98426473]\n",
            "[0.0338784 0.9661216]\n",
            "[0.01743693 0.98256314]\n",
            "[0.9231106  0.07688943]\n",
            "[0.9287215  0.07127851]\n",
            "[0.0128075 0.9871925]\n",
            "[0.28737688 0.7126232 ]\n",
            "[0.0128075 0.9871925]\n",
            "[0.03811106 0.9618889 ]\n",
            "[0.9114444  0.08855565]\n",
            "[0.92015564 0.07984435]\n",
            "[0.9186396  0.08136038]\n",
            "[0.9170974  0.08290263]\n",
            "[0.9147192  0.08528076]\n",
            "[0.8923662  0.10763384]\n",
            "[0.06800118 0.9319988 ]\n",
            "[0.9245501  0.07544986]\n",
            "[0.0301011 0.9698989]\n",
            "[0.09522419 0.9047758 ]\n",
            "[0.9163152  0.08368482]\n",
            "[0.06800118 0.9319988 ]\n",
            "[0.690293   0.30970693]\n",
            "[0.01743693 0.98256314]\n",
            "[0.63581705 0.36418295]\n",
            "[0.01155225 0.9884477 ]\n",
            "[0.31301937 0.68698066]\n",
            "[0.08521038 0.9147896 ]\n",
            "[0.9245501  0.07544986]\n",
            "[0.01573521 0.98426473]\n",
            "[0.88110024 0.11889981]\n",
            "[0.13180414 0.86819583]\n",
            "[0.92383343 0.07616653]\n",
            "[0.31301937 0.68698066]\n",
            "[0.03811106 0.9618889 ]\n",
            "[0.21848594 0.7815141 ]\n",
            "[0.01937912 0.9806209 ]\n",
            "[0.17964612 0.82035387]\n",
            "[0.01155225 0.9884477 ]\n",
            "[0.48666963 0.5133304 ]\n",
            "[0.90894824 0.09105177]\n",
            "[0.92666316 0.07333687]\n",
            "[0.0540622 0.9459379]\n",
            "[0.7399531  0.26004687]\n",
            "[0.03811106 0.9618889 ]\n",
            "[0.92383343 0.07616653]\n",
            "[0.9195102  0.08048978]\n",
            "[0.92596495 0.07403506]\n",
            "[0.8923662  0.10763384]\n",
            "[0.51718897 0.48281103]\n",
            "[0.9114444  0.08855565]\n",
            "[0.24005216 0.75994784]\n",
            "[0.04814665 0.95185333]\n",
            "[0.17964612 0.82035387]\n",
            "[0.14641725 0.85358274]\n",
            "[0.24005216 0.75994784]\n",
            "[0.13180414 0.86819583]\n",
            "[0.01573521 0.98426473]\n",
            "[0.0128075 0.9871925]\n",
            "[0.9254179 0.0745821]\n",
            "[0.91787183 0.0821282 ]\n",
            "[0.9194009  0.08059911]\n",
            "[0.9122735  0.08772642]\n",
            "[0.92383343 0.07616653]\n",
            "[0.48666963 0.5133304 ]\n",
            "[0.8397424  0.16025752]\n",
            "[0.39659154 0.6034085 ]\n",
            "[0.9223814  0.07761861]\n",
            "[0.24005216 0.75994784]\n",
            "[0.928792 0.071208]\n",
            "[0.48666963 0.5133304 ]\n",
            "[0.9151611  0.08483896]\n",
            "[0.9231106  0.07688943]\n",
            "[0.92666316 0.07333687]\n",
            "[0.10627808 0.89372194]\n",
            "[0.31301937 0.68698066]\n",
            "[0.06800118 0.9319988 ]\n",
            "[0.92666316 0.07333687]\n",
            "[0.36776778 0.63223225]\n",
            "[0.07616099 0.92383903]\n",
            "[0.01743693 0.98256314]\n",
            "[0.91787183 0.0821282 ]\n",
            "[0.76275486 0.23724514]\n",
            "[0.8397424  0.16025752]\n",
            "[0.04814665 0.95185333]\n",
            "[0.90894824 0.09105177]\n",
            "[0.24005216 0.75994784]\n",
            "[0.10627808 0.89372194]\n",
            "[0.03811106 0.9618889 ]\n",
            "[0.39659154 0.6034085 ]\n",
            "[0.92383343 0.07616653]\n",
            "[0.31301937 0.68698066]\n",
            "[0.07616099 0.92383903]\n",
            "[0.09522419 0.9047758 ]\n",
            "[0.1184472 0.8815528]\n",
            "[0.0301011 0.9698989]\n",
            "[0.63581705 0.36418295]\n",
            "[0.63581705 0.36418295]\n",
            "[0.03811106 0.9618889 ]\n",
            "[0.90160155 0.09839846]\n",
            "[0.10627808 0.89372194]\n",
            "[0.16234756 0.8376525 ]\n",
            "[0.1184472 0.8815528]\n",
            "[0.01743693 0.98256314]\n",
            "[0.16234756 0.8376525 ]\n",
            "[0.9280414  0.07195864]\n",
            "[0.9170974  0.08290263]\n",
            "[0.91787183 0.0821282 ]\n",
            "[0.8226193  0.17738065]\n",
            "[0.9287215  0.07127851]\n",
            "[0.8226193  0.17738065]\n",
            "[0.90160155 0.09839846]\n",
            "[0.4562491  0.54375094]\n",
            "[0.92735523 0.07264473]\n",
            "[0.9254179 0.0745821]\n",
            "[0.91552067 0.08447936]\n",
            "[0.02414715 0.9758529 ]\n",
            "[0.91552067 0.08447936]\n",
            "[0.31301937 0.68698066]\n",
            "[0.9225497  0.07745028]\n",
            "[0.91787183 0.0821282 ]\n",
            "[0.33985892 0.66014105]\n",
            "[0.01937912 0.9806209 ]\n",
            "[0.01155225 0.9884477 ]\n",
            "[0.76275486 0.23724514]\n",
            "[0.0338784 0.9661216]\n",
            "[0.90894824 0.09105177]\n",
            "[0.07616099 0.92383903]\n",
            "[0.92596495 0.07403506]\n",
            "[0.24005216 0.75994784]\n",
            "[0.9114444  0.08855565]\n",
            "[0.02414715 0.9758529 ]\n",
            "[0.9287215  0.07127851]\n",
            "[0.39659154 0.6034085 ]\n",
            "[0.03811106 0.9618889 ]\n",
            "[0.04814665 0.95185333]\n",
            "[0.16234756 0.8376525 ]\n",
            "[0.92596495 0.07403506]\n",
            "[0.91787183 0.0821282 ]\n",
            "[0.39659154 0.6034085 ]\n",
            "[0.1184472 0.8815528]\n",
            "[0.92735523 0.07264473]\n",
            "[0.8040935  0.19590652]\n",
            "[0.1184472 0.8815528]\n",
            "[0.8923662  0.10763384]\n",
            "[0.92596495 0.07403506]\n",
            "[0.8554976 0.1445024]\n",
            "[0.9254179 0.0745821]\n",
            "[0.9280414  0.07195864]\n",
            "[0.39659154 0.6034085 ]\n",
            "[0.48666963 0.5133304 ]\n",
            "[0.09522419 0.9047758 ]\n",
            "[0.36776778 0.63223225]\n",
            "[0.28737688 0.7126232 ]\n",
            "[0.9194009  0.08059911]\n",
            "[0.07616099 0.92383903]\n",
            "[0.9194009  0.08059911]\n",
            "[0.92666316 0.07333687]\n",
            "[0.92666316 0.07333687]\n",
            "[0.92383343 0.07616653]\n",
            "[0.08521038 0.9147896 ]\n",
            "[0.76275486 0.23724514]\n",
            "[0.17964612 0.82035387]\n",
            "[0.9280414  0.07195864]\n",
            "[0.17964612 0.82035387]\n",
            "[0.63581705 0.36418295]\n",
            "[0.03811106 0.9618889 ]\n",
            "[0.31301937 0.68698066]\n",
            "[0.9225497  0.07745028]\n",
            "[0.0128075 0.9871925]\n",
            "[0.01573521 0.98426473]\n",
            "[0.60709953 0.39290047]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4zb4jyPRPHP"
      },
      "source": [
        "预测列表中的每个元素本身就是一个长度为2的列表。每个列表中两个值的总和为1。这是因为两列包含每种可能输出的概率：经历过的副作用和没有经历过的副作用。 预测列表中的每个元素都是所有可能输出上的概率分布。\r\n",
        "\r\n",
        "第一列包含每个患者没有出现副作用的概率，用0表示。第二列包含每个患者发生副作用的概率，用1表示。\r\n",
        "\r\n",
        "我们也可以只看最可能的预测。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "quhlRVmCRDIZ",
        "outputId": "bc12d3f5-3428-43c2-d1a5-8c7489e33876"
      },
      "source": [
        "rouunded_predictions = np.argmax(predictions,axis=-1)\r\n",
        "for i in rouunded_predictions:\r\n",
        "  print(i)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyWrVMcNRqYX"
      },
      "source": [
        "从打印的预测结果中，我们可以观察模型中潜在的预测，但是，我们不能仅仅通过查看预测的输出来判断这些预测有多准确。\r\n",
        "\r\n",
        "如果我们具有测试集的相应标签（在这种情况下，我们这样做），则可以将这些真实标签与预测标签进行比较，以判断模型评估的准确性。 在下一集中，我们将看到如何使用称为混淆矩阵的工具将其可视化。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zZrygK4RlTw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}